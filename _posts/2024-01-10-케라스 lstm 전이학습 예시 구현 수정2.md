# import 


```python
from datasetsforecast.m4 import M4, M4Evaluation
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import time
```

# pretrain data

- m4 hourly data
- 24주기 window로 나타냄
- (363850, 23) 


```python
df,*_= M4.load(directory='data',group = 'Hourly')

lst = list(set(df.loc[:,'unique_id']))

tt = [df[df.loc[:,'unique_id']==lst[i]].iloc[:,1:4].set_index(['ds']) 
      for i in range(len(lst))]

for l in range(len(tt)):
    for s in range(1, 24):
        tt[l]['shift_{}'.format(s)] = tt[l]['y'].shift(s)
        tt[l]['shift_{}'.format(s)] = tt[l]['y'].shift(s)
        
tt=[tt[i].dropna(axis=0) for i in range(len(tt))]
```


```python
train = np.concatenate([np.array(tt[i].iloc[:,1:]) for i in range(len(tt))])
y = np.concatenate([np.array(tt[i].iloc[:,0]) for i in range(len(tt))]).reshape(-1,1)
```


```python
from sklearn.preprocessing import MinMaxScaler
min_max_scaler1 = MinMaxScaler()

X_scale = min_max_scaler1.fit_transform(train)
y_scale = min_max_scaler1.fit_transform(y)
```

# target data

- 건물의 전력소비량 벡터, 24주기
- (183579, 23, 1)
- 이중에서 끝에서부터 2400개의 데이터만 활용 (target data의 크기가 훨씬 적다고 보기 위해)


```python
df = pd.read_csv('C:/Users/default.DESKTOP-2ISHQBS/Documents/R/time_ele/train.csv')

arr = df.iloc[:,9] # 전력소비량
date=  pd.to_datetime(df.iloc[:,2]) # 일시

df_= pd.DataFrame({'date':date,
              'ele': arr})

df_ = df_.set_index('date')

from sklearn.preprocessing import MinMaxScaler
min_max_scaler = MinMaxScaler()

df_['mmele'] = min_max_scaler.fit_transform(df_.iloc[:].to_numpy().reshape(-1,1))

df_ = df_.drop(['ele'],axis=1)

for s in range(1, 24):
    df_['shift_{}'.format(s)] = df_['mmele'].shift(s)
    df_['shift_{}'.format(s)] = df_['mmele'].shift(s)

df_ = df_.dropna()

y = df_.iloc[:,[0]] #scaled 

X = df_.iloc[:,1:]

from sklearn.model_selection import train_test_split

x_train,x_test, y_train , y_test = train_test_split(X,y,shuffle=False, test_size=0.1)

x_train_ = x_train.to_numpy().reshape(x_train.shape[0],x_train.shape[1],1)

x_test_ = x_test.to_numpy().reshape(x_test.shape[0],x_test.shape[1],1)
```


```python
x_train_.shape
```




    (183579, 23, 1)



---

# pretrained Keras lstm


```python
from keras.layers import LSTM
from keras.models import Sequential
from keras.layers import Dense
import keras.backend as K
from keras.callbacks import EarlyStopping
```


```python
K.clear_session()
model1 = Sequential() # Sequeatial Model
model1.add(LSTM(24, input_shape=(23, 1),activation='linear')) # (timestep, feature)
model1.add(Dense(64,activation='linear'))
model1.add(Dense(64,activation='linear'))
model1.add(Dense(1)) # output = 1
model1.compile(loss='mean_squared_error', optimizer='adam')
```


```python
np.random.seed(1)
early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)

model1.fit(X_scale, y_scale, epochs=100,
          batch_size=128, verbose=1, callbacks=[early_stop])

```

    Epoch 1/100
    2843/2843 [==============================] - 12s 4ms/step - loss: 6.5057e-05
    Epoch 2/100
    2843/2843 [==============================] - 11s 4ms/step - loss: 6.9916e-06
    Epoch 3/100
    2843/2843 [==============================] - 11s 4ms/step - loss: 8.3092e-06
    Epoch 4/100
    2843/2843 [==============================] - 11s 4ms/step - loss: 8.3490e-06
    Epoch 5/100
    2843/2843 [==============================] - 11s 4ms/step - loss: 7.9074e-06
    Epoch 5: early stopping
    




    <keras.src.callbacks.History at 0x19cdbd95030>



---

# 1. no pretrained lstm model0

- 전이학습과의 성능 비교를 위한 모델
- target data만을 가지고 모델 피팅


```python
K.clear_session()
model0 = Sequential() # Sequeatial Model
model0.add(LSTM(24, input_shape=(23, 1),activation='linear')) # (timestep, feature)
model0.add(Dense(64,activation='linear'))
model0.add(Dense(64,activation='linear'))
model0.add(Dense(1)) # output = 1
model0.compile(loss='mean_squared_error', optimizer='adam')
```


```python
early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)
start_time0 = time.time()
model0.fit(x_train_, y_train, epochs=100,
          batch_size=128, verbose=1, callbacks=[early_stop])
end_time0 = time.time()
```

    Epoch 1/100
    1435/1435 [==============================] - 7s 4ms/step - loss: 5.7378e-04
    Epoch 2/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 2.8423e-04
    Epoch 3/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 2.5496e-04
    Epoch 4/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 2.3243e-04
    Epoch 5/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 2.2185e-04
    Epoch 6/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 2.1106e-04
    Epoch 7/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 2.0863e-04
    Epoch 8/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 2.0059e-04
    Epoch 9/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.8272e-04
    Epoch 10/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.7895e-04
    Epoch 11/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.6643e-04
    Epoch 12/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.6019e-04
    Epoch 13/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.5408e-04
    Epoch 14/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.5100e-04
    Epoch 15/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.4594e-04
    Epoch 16/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.4260e-04
    Epoch 17/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.4170e-04
    Epoch 18/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.3459e-04
    Epoch 19/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.3280e-04
    Epoch 20/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.2538e-04
    Epoch 21/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.2578e-04
    Epoch 22/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.2448e-04
    Epoch 23/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.1808e-04
    Epoch 24/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.1509e-04
    Epoch 25/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.1220e-04
    Epoch 26/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.0790e-04
    Epoch 27/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.0563e-04
    Epoch 28/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.0662e-04
    Epoch 29/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 1.0070e-04
    Epoch 30/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 9.8024e-05
    Epoch 31/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 9.8836e-05
    Epoch 32/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 9.6006e-05
    Epoch 33/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 9.3325e-05
    Epoch 34/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 9.3318e-05
    Epoch 35/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 9.1110e-05
    Epoch 36/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 9.2228e-05
    Epoch 37/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.6137e-05
    Epoch 38/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.8056e-05
    Epoch 39/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.5458e-05
    Epoch 40/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.4670e-05
    Epoch 41/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.5152e-05
    Epoch 42/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.3389e-05
    Epoch 43/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.1793e-05
    Epoch 44/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.2799e-05
    Epoch 45/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.0002e-05
    Epoch 46/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.9963e-05
    Epoch 47/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 8.0020e-05
    Epoch 48/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.9515e-05
    Epoch 49/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.7948e-05
    Epoch 50/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.7181e-05
    Epoch 51/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.7077e-05
    Epoch 52/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.7938e-05
    Epoch 53/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.4928e-05
    Epoch 54/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.5680e-05
    Epoch 55/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.3158e-05
    Epoch 56/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.5455e-05
    Epoch 57/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.3397e-05
    Epoch 58/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.1238e-05
    Epoch 59/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.2221e-05
    Epoch 60/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.2856e-05
    Epoch 61/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.0761e-05
    Epoch 62/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.0991e-05
    Epoch 63/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.0276e-05
    Epoch 64/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.0117e-05
    Epoch 65/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.8904e-05
    Epoch 66/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 7.0521e-05
    Epoch 67/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.8753e-05
    Epoch 68/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.9150e-05
    Epoch 69/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.8782e-05
    Epoch 70/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.6572e-05
    Epoch 71/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.6920e-05
    Epoch 72/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.8490e-05
    Epoch 73/100
    1435/1435 [==============================] - 6s 4ms/step - loss: 6.7188e-05
    Epoch 73: early stopping
    


```python
total_training_time0 = end_time0 - start_time0

print(f"총 학습에 걸린 시간: {total_training_time0} 초")
```

    총 학습에 걸린 시간: 420.26451659202576 초
    


```python
model1.summary()
```

    Model: "sequential"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     lstm (LSTM)                 (None, 24)                2496      
                                                                     
     dense (Dense)               (None, 64)                1600      
                                                                     
     dense_1 (Dense)             (None, 64)                4160      
                                                                     
     dense_2 (Dense)             (None, 1)                 65        
                                                                     
    =================================================================
    Total params: 8321 (32.50 KB)
    Trainable params: 8321 (32.50 KB)
    Non-trainable params: 0 (0.00 Byte)
    _________________________________________________________________
    

---

# 2.1. transfer learning all freezing model21

- base model 전체를 사용, freezing


```python
from tensorflow.keras import layers
from tensorflow.keras import applications
```


```python
for layer in model1.layers:
    layer.trainable = False # freezing

model21 = Sequential()
model21.add(model1.layers[0]) # pretrain model inputlayer1
model21.add(model1.layers[1]) # pretrain model hidden layer2
model21.add(model1.layers[2]) # pretrain model hidden layer3
model21.add(model1.layers[3])
model21.add(layers.Dense(64, activation='linear'))
model21.add(layers.Dense(1, activation='linear'))
model21.compile(loss='mean_squared_error', optimizer='adam')
```


```python
early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)
start_time21 = time.time()
model21.fit(x_train_[:2400], y_train[:2400], epochs=100,
          batch_size=128, verbose=1, callbacks=[early_stop])
end_time21 = time.time()
```

    Epoch 1/100
    19/19 [==============================] - 1s 3ms/step - loss: 0.0035
    Epoch 2/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0018
    Epoch 3/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0016
    Epoch 4/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0015
    Epoch 5/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0015
    Epoch 6/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0014
    Epoch 7/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0013
    Epoch 8/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0012
    Epoch 9/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0012
    Epoch 10/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0011
    Epoch 11/100
    19/19 [==============================] - 0s 3ms/step - loss: 0.0010
    Epoch 12/100
    19/19 [==============================] - 0s 3ms/step - loss: 9.4119e-04
    Epoch 13/100
    19/19 [==============================] - 0s 3ms/step - loss: 8.7291e-04
    Epoch 14/100
    19/19 [==============================] - 0s 3ms/step - loss: 8.2263e-04
    Epoch 15/100
    19/19 [==============================] - 0s 3ms/step - loss: 7.7559e-04
    Epoch 16/100
    19/19 [==============================] - 0s 3ms/step - loss: 7.4621e-04
    Epoch 17/100
    19/19 [==============================] - 0s 3ms/step - loss: 7.1110e-04
    Epoch 18/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.8803e-04
    Epoch 19/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.7150e-04
    Epoch 20/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.5773e-04
    Epoch 21/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.5476e-04
    Epoch 22/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.4864e-04
    Epoch 23/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.4975e-04
    Epoch 24/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.4541e-04
    Epoch 25/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.3997e-04
    Epoch 26/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.3755e-04
    Epoch 27/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.3874e-04
    Epoch 28/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.4500e-04
    Epoch 29/100
    19/19 [==============================] - 0s 3ms/step - loss: 6.4601e-04
    Epoch 29: early stopping
    


```python
total_training_time21 = end_time21 - start_time21

print(f"총 학습에 걸린 시간: {total_training_time21} 초")
```

    총 학습에 걸린 시간: 2.3292884826660156 초
    

---

# 2.2 transfer learning non freezing model22
- base model의 모든 히든레이어를 사용, 프리징하지 않음


```python
for layer in model1.layers:
    layer.trainable = True # freezing

model22 = Sequential()
model22.add(model1.layers[0]) # pretrain model input layer1
model22.add(model1.layers[1]) # pretrain model hidden layer2
model22.add(model1.layers[2]) # pretrain model hidden layer3
model22.add(model1.layers[3])
model22.add(layers.Dense(64, activation='linear'))
model22.add(layers.Dense(1, activation='linear'))
model22.compile(loss='mean_squared_error', optimizer='adam')
```


```python
early_stop = EarlyStopping(monitor='loss', patience=3, verbose=1)
start_time22 = time.time()
model22.fit(x_train_[:2400], y_train[:2400], epochs=100,
          batch_size=128, verbose=1, callbacks=[early_stop])
end_time22 = time.time()
```

    Epoch 1/100
    19/19 [==============================] - 1s 4ms/step - loss: 0.0038
    Epoch 2/100
    19/19 [==============================] - 0s 4ms/step - loss: 0.0020
    Epoch 3/100
    19/19 [==============================] - 0s 4ms/step - loss: 0.0019
    Epoch 4/100
    19/19 [==============================] - 0s 4ms/step - loss: 0.0014
    Epoch 5/100
    19/19 [==============================] - 0s 4ms/step - loss: 4.4351e-04
    Epoch 6/100
    19/19 [==============================] - 0s 4ms/step - loss: 3.1861e-04
    Epoch 7/100
    19/19 [==============================] - 0s 4ms/step - loss: 2.9169e-04
    Epoch 8/100
    19/19 [==============================] - 0s 4ms/step - loss: 2.7229e-04
    Epoch 9/100
    19/19 [==============================] - 0s 4ms/step - loss: 2.5533e-04
    Epoch 10/100
    19/19 [==============================] - 0s 4ms/step - loss: 2.4443e-04
    Epoch 11/100
    19/19 [==============================] - 0s 4ms/step - loss: 2.3480e-04
    Epoch 12/100
    19/19 [==============================] - 0s 4ms/step - loss: 2.3241e-04
    Epoch 13/100
    19/19 [==============================] - 0s 4ms/step - loss: 2.0175e-04
    Epoch 14/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.8626e-04
    Epoch 15/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.7979e-04
    Epoch 16/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.6951e-04
    Epoch 17/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.6461e-04
    Epoch 18/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.5625e-04
    Epoch 19/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.5953e-04
    Epoch 20/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.5396e-04
    Epoch 21/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.4664e-04
    Epoch 22/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.4773e-04
    Epoch 23/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.3995e-04
    Epoch 24/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.4629e-04
    Epoch 25/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.4970e-04
    Epoch 26/100
    19/19 [==============================] - 0s 4ms/step - loss: 1.5101e-04
    Epoch 26: early stopping
    


```python
total_training_time22 = end_time22 - start_time22

print(f"총 학습에 걸린 시간: {total_training_time22} 초")
```

    총 학습에 걸린 시간: 3.375065565109253 초
    

---

# test


```python
pred0 = model0.predict(x_test_)
pred21 = model21.predict(x_test_)
pred22 = model22.predict(x_test_)
#pred31 = model31.predict(x_test_)
#pred32 = model32.predict(x_test_)
```

    638/638 [==============================] - 1s 1ms/step
    638/638 [==============================] - 1s 1ms/step
    638/638 [==============================] - 1s 1ms/step
    638/638 [==============================] - 1s 1ms/step
    638/638 [==============================] - 1s 1ms/step
    


```python

#print('layer_not_pretrain MSE :',format(MSE(pred0.flatten(),y_test.values.flatten()),'f'))
#print('layer_all_freezing MSE :',format(MSE(pred21.flatten(),y_test.values.flatten()),'f'))
#print('layer_not_freezing MSE :',format(MSE(pred22.flatten(),y_test.values.flatten()),'f'))
#print('lastlayer_use_freezing MSE:',format(MSE(pred31.flatten(),y_test.values.flatten()),'f'))
#print('lastlayer_not_freezing MSE:',format(MSE(pred32.flatten(),y_test.values.flatten()),'f'))
#print('layer_not_pretrain MAE :',format(MAE(pred0.flatten(),y_test.values.flatten()),'f'))
#print('layer_all_freezing MAE :',format(MAE(pred21.flatten(),y_test.values.flatten()),'f'))
#print('layer_not_freezing MAE :',format(MAE(pred22.flatten(),y_test.values.flatten()),'f'))
#print('lastlayer_use_freezing MAE:',format(MAE(pred31.flatten(),y_test.values.flatten()),'f'))
#print('lastlayer_not_freezing MAE:',format(MAE(pred32.flatten(),y_test.values.flatten()),'f'))
```


```python
n =100
plt.plot(pred0[:n],label = 'layer_not_pretrain')
plt.plot(pred21[:n],label = 'layer_all_freezing')
plt.plot(pred22[:n],label = 'layer_not_freezing',color = 'red')
#plt.plot(pred31[:n],label = 'last_not_freezing')
#plt.plot(pred32[:n],label = 'last_not_freezing',color='brown')
plt.plot(y_test.values.flatten()[:n],label = 'observed',color = 'black')
plt.legend(loc='lower left')
plt.show()
```


    
![png](output_36_0.png)
    


---

# trained time, MSE table


```python
def MSE(y_pred, y_true):
    return np.mean((y_true - y_pred)**2)


def MAE(y_pred, y_true):
    absolute_errors = np.abs(y_true - y_pred)
    mae = np.mean(absolute_errors)
    return mae

def MAPE(y_true, y_pred):
    epsilon = 1e-10
    percentage_errors = np.abs((y_true - y_pred) / (y_true + epsilon)) * 100
    percentage_errors = np.nan_to_num(percentage_errors)
    mape = np.mean(percentage_errors)  
    return mape
```


```python
y_pred = y_test.values.flatten()

lst1 = total_training_time0,total_training_time21,total_training_time22
lst2 =  MSE(pred0.flatten(),y_test.values.flatten()), MSE(pred21.flatten(),y_test.values.flatten()),MSE(pred22.flatten(),y_test.values.flatten())
lst3 = MAE(pred0.flatten(),y_test.values.flatten()), MAE(pred21.flatten(),y_test.values.flatten()), MAE(pred22.flatten(),y_test.values.flatten())
lst4 =  MAPE(pred0.flatten(),y_test.values.flatten()), MAPE(pred21.flatten(),y_test.values.flatten()), MAPE(pred22.flatten(),y_test.values.flatten())

name = ['no_prtrain','all_use_freezing','all_not_freezing']
```


```python
pd.options.display.float_format = '{:.7f}'.format
pd.DataFrame({'time(second)':lst1,
             'MSE':lst2,
              'MAE':lst3,
             'MAPE':lst4},index=name)
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>time(second)</th>
      <th>MSE</th>
      <th>MAE</th>
      <th>MAPE</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>no_prtrain</th>
      <td>420.2645166</td>
      <td>0.0000382</td>
      <td>0.0048069</td>
      <td>9.4338063</td>
    </tr>
    <tr>
      <th>all_use_freezing</th>
      <td>2.3292885</td>
      <td>1.8974709</td>
      <td>1.2453748</td>
      <td>118.3953502</td>
    </tr>
    <tr>
      <th>all_not_freezing</th>
      <td>3.3750656</td>
      <td>0.0000703</td>
      <td>0.0061820</td>
      <td>16.7945700</td>
    </tr>
  </tbody>
</table>
</div>



- 전이학습으로 학습한 모델은 매우 빠른 학습시간을 보여준다
- MSE, MAE, MAPE 지표는 target data를 fitting한 모델이 우수하다. 그러나 학습시간대비 전이학습모델 또한 성능이 매우 훌륭함
- 모든 레이어를 freezing한 model은 다른 모델과 달리 유의하게 성능이 뒤떨어지는 것을 확인
